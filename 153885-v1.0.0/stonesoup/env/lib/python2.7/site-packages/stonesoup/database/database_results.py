import argparse
import os
import sys
import csv
import xlsxwriter
import signal
import time
import threading
import time
from xlsxwriter.utility import xl_rowcol_to_cell
from stonesoup.database import *
from stonesoup.database.utils import database, queries
from progressbar import Bar, Percentage, \
    ProgressBar, RotatingMarker, AdaptiveETA, Timer, ETA, FileTransferSpeed, \
    AnimatedMarker
from datetime import datetime, timedelta

local_module = sys.modules[__name__]

analyze_headers = ['Test Case Name',
                    'Analysis Stage',
                    'Run ID',
                    'Run Time',
                    'CWE Found',
                    'CWEs Found',
                    'Impact Found',
                    'Impacts Found',
                    'Result',
                    'Error']

execute_headers = ['Test Case Name',
                    'Analysis Stage',
                    'Execute Stage',
                    'IOPair Name',
                    'IOPair Type',
                    'Analysis ID',
                    'Run ID',
                    'Analysis Time',
                    'Run Time',
                    'External Run Time',
                    'Internal Run Time',
                    'CWE Found',
                    'CWEs Found',
                    'Impact Found',
                    'Impacts Found',
                    'Timed Out',
                    'Score Type',
                    'Score Name',
                    'Check',
                    'Result',
                    'Error',
                    'Modified Scoring']

perf_headers = ['Test Case Name',
                  'Execute Stage',
                  'IOPair Name',
                  'IOPair Type',
                  'Analysis ID',
                  'Run ID',
                  'Run Time',
                  'Timed Out',
                  'Point Type',
                  'Point Ordinal',
                  'Point Name',
                  'Point Timestamp',
                  'Point Delta']

class DownloadTimer(threading.Thread):
    def __init__(self, **kwargs):
        threading.Thread.__init__(self)
        self.fd = kwargs.pop('timer_fd', None)
        self.daemon = True
        self._stop = threading.Event()
        self._stopping = threading.Event()
        self._widget_string = kwargs.pop("widget_string", "Downloading")

    def run(self):
        self._stop.clear()

        widgets = ['{}: '.format(self._widget_string), AnimatedMarker(), ' ', Timer()]
        pbar = ProgressBar(widgets=widgets).start()
        while not self._stop.isSet():
            time.sleep(.5)
            pbar.update()
        pbar.finish()

    def stop(self):
        self._stop.set()
        time.sleep(2)


def get_default_widgets():
    widgets = ['Percentage: ', Percentage(), ' ',
               Bar(marker='0', left='[', right=']'),
               ' ', ETA(), ' ', FileTransferSpeed()]

    return widgets



class NotImplementedError(Exception):
    """
    InputError Execption for bad commands
    """
    def __init__(self, value):
        self.value = value

    def __str__(self):
        return repr(self.value)


def subcommand_args_not_implemented(parser):
    """raises an error that the parser is not implemented when called
    args:
        parser: an argparse argumentparser
    returns:
        none.
    raises:
        notimplementederror: sub-command not implemented.
    """
    raise NotImplementedError(
        'sub-command not implemented.'), None, sys.exc_info()[2]



def subcommand_args_common(parser):
    parser.add_argument("-i", "--ipaddress", dest="IP",
                        help="IP address of database server", required=True)
    parser.add_argument("-p", "--port", dest="PORT",
                        help="Port Number of Database Server", default="27017")

    parser.add_argument('-v', '--verbose', dest="verbose", action="store_true",
                        required=False,default=False,
                        help="run in verbose mode.")
    parser.add_argument('--valid', dest='cull_valid', action='store_true',
                        required=False, default=False, help='return only results\
                         for stage 2 that have a corresponding passing score in stage 1')
    parser.add_argument("-o", "--output", dest="output", default="./",
                        help="folder to output results files")


def subcommand_args_xlsx(sub_parsers):
    parser = sub_parsers.add_parser("xlsx",
                                    help="Get calculated scoring results from a queue or all queues.",
                                    description="Get calculated scoring results from a queue or all queues.")

    subcommand_args_common(parser)

    parser.add_argument("-q", "--queue", dest="queue", required=True, help="Names of queue to pull scoring results from.")
    parser.add_argument("--perf", dest="performance", required=False,
                        default=False, action="store_true",
                        help="Get performance results of the queue's runs.")
    parser.add_argument("--testcases", dest="testcases", required=False,
                        default=None, nargs="+",
                        help="Specific testcases to find performance results for. (Requires --perf)")
    parser.add_argument("--type", dest="type", required=False,
                        choices=["passed","failed","both"], default="passed",
                        help="Types of runs to pull (Requires --perf, Default: passed)")
    parser.set_defaults(handler=subcommand_xlsx)

def subcommand_xlsx(args):
    """
    Create a comprehensive spreadsheet that cuts the data about a million different ways
    """
    init_db(args)
    init_output(args)
    queue = DbQueue.get_queue(args.queue)

    if queue is None:
        print "Queue '{}' does not exist on the database. Exiting."
        return

    stages, testcases = queries.get_calculated_results(queue, cull=args.cull_valid)


    workbook = xlsxwriter.Workbook(os.path.join(args.output, '{}.xlsx'.format(queue.queue_name)))
    formats = create_formats(workbook)

    #actually write sheets
    write_xlsx_summary(args, workbook, formats, stages, testcases)

    if args.performance:
        write_perf_results(args, workbook, formats, queue)
    workbook.close()

def create_formats(workbook):
    formats = {}
    formats['MA_HEADER'] = workbook.add_format({ 'bold' : True, 'align' : 'center'})
    formats['ALTERED_HEADER'] = workbook.add_format({ 'bold': True, 'bg_color' : '#7030A0', 'font_color' : 'white', 'align' : 'center','border' : 1})
    formats['MITIGATED_HEADER'] = workbook.add_format({ 'bold': True, 'bg_color' : '#16365C', 'font_color' : 'white', 'align' : 'center', 'valign' : 'vcenter','border' : 1})
    formats['TOTAL_HEADER'] = workbook.add_format({ 'bold': True, 'bg_color' : '#4F6228', 'font_color' : 'white', 'align' : 'center','border' : 1})
    formats['FULL/UNALTERED'] = workbook.add_format({ 'bg_color' : '#00CC00', 'align' : 'center','border' : 1})
    formats['FULL/ALTERED'] = workbook.add_format({ 'bg_color' : '#FFCC66', 'align' : 'center','border' : 1})
    formats['PARTIAL/UNALTERED'] = workbook.add_format({ 'bg_color' : '#99FF66', 'align' : 'center','border' : 1})
    formats['PARTIAL/ALTERED'] = workbook.add_format({ 'bg_color' : '#FF9966', 'align' : 'center','border' : 1})
    formats['NONE/UNALTERED'] = workbook.add_format({ 'bg_color' : '#FFFF99', 'align' : 'center','border' : 1})
    formats['NONE/ALTERED'] = workbook.add_format({ 'bg_color' : '#FF5050', 'align' : 'center','border' : 1})

    formats['FULL/UNALTERED_PCT'] = workbook.add_format({ 'bg_color' : '#00CC00', 'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['FULL/ALTERED_PCT'] = workbook.add_format({ 'bg_color' : '#FFCC66', 'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['PARTIAL/UNALTERED_PCT'] = workbook.add_format({ 'bg_color' : '#99FF66', 'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['PARTIAL/ALTERED_PCT'] = workbook.add_format({ 'bg_color' : '#FF9966', 'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['NONE/UNALTERED_PCT'] = workbook.add_format({ 'bg_color' : '#FFFF99', 'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['NONE/ALTERED_PCT'] = workbook.add_format({ 'bg_color' : '#FF5050', 'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['NORMAL_PCT'] = workbook.add_format({'align' : 'center', 'num_format' : '0.00','border' : 1})
    formats['NORMAL'] = workbook.add_format({'align' : 'center','border' : 1})
    formats['TIMESTAMP'] = workbook.add_format({'num_format' : '0#.####0'})
    return formats

def write_perf_results(args, workbook, formats, queue):
    testcase_list = get_testcases(args, queue)

    performance_results = get_performance_results(queue, testcase_list)
    if len(performance_results) == 0:
        print "No results found on database."
        return
    organized_results = organize_results(performance_results)
    calculate_spreadsheet(organized_results)

    write_spreadsheet(workbook, formats, organized_results)


def write_xlsx_summary(args, workbook, formats, stages, testcases):
    """
        Write out overall results for
    """

    widgets = ['Writing Summary Sheet: ', Timer(), ' ']
    pbar = ProgressBar(widgets=widgets, maxval=len(testcases)).start()
    summary_sheet = workbook.add_worksheet('Summary')

    #Stage 1
    summary_sheet.merge_range('A1:K1', 'Stage 1', formats['MA_HEADER'] )
    create_alteration_mitigation_table(formats, summary_sheet, 1, 0, stages['1'])
    create_alteration_mitigation_table(formats, summary_sheet, 1, 6, stages['1'], percentage=True)

    #Stage 2
    summary_sheet.merge_range('A9:K9', 'Stage 2', formats['MA_HEADER'] )
    create_alteration_mitigation_table(formats, summary_sheet, 10, 0, stages['2'])
    create_alteration_mitigation_table(formats, summary_sheet, 10, 6, stages['2'], percentage=True)

    pbar.finish()


def create_alteration_mitigation_table(formats, worksheet, row, col, result, percentage=False):
    """
    Takes a result dictionary and returns
    """

    #Altered Header
    worksheet.merge_range(row, col+2, row, col+3, 'Altered?', formats['ALTERED_HEADER'])
    worksheet.write(row+1, col+2, 'No', formats['ALTERED_HEADER'])
    worksheet.write(row+1, col+3, 'Yes', formats['ALTERED_HEADER'])

    #Mitigated Header
    worksheet.merge_range(row+2, col, row+4, col, 'Mitigated?', formats['MITIGATED_HEADER'])
    worksheet.write(row+2, col+1, 'Full', formats['MITIGATED_HEADER'])
    worksheet.write(row+3, col+1, 'Partial', formats['MITIGATED_HEADER'])
    worksheet.write(row+4, col+1, 'None', formats['MITIGATED_HEADER'])

    #Total Headers
    worksheet.write(row+5, col+1, 'Total', formats['TOTAL_HEADER'])
    worksheet.write(row+1, col+4, 'Total', formats['TOTAL_HEADER'])


    #Write values
    #Full/Unaltered
    worksheet.write(row+2, col+2, _get_value_or_percentage(result, 'full_unaltered', percentage) , formats['FULL/UNALTERED{}'.format('_PCT' if percentage else '')])
    #Full/Altered
    worksheet.write(row+2, col+3, _get_value_or_percentage(result,'full_altered', percentage), formats['FULL/ALTERED{}'.format('_PCT' if percentage else '')])
    #Full Total
    worksheet.write_formula(row+2, col+4, '{{=SUM({}:{})}}'.format(xl_rowcol_to_cell(row+2, col+2), xl_rowcol_to_cell(row+2, col+3)), formats['NORMAL{}'.format('_PCT' if percentage else '')])


    #Partial/Unaltered
    worksheet.write(row+3, col+2, _get_value_or_percentage(result, 'partial_unaltered', percentage), formats['PARTIAL/UNALTERED{}'.format('_PCT' if percentage else '')])
    #Partial/Altered
    worksheet.write(row+3, col+3, _get_value_or_percentage(result, 'partial_altered', percentage), formats['PARTIAL/ALTERED{}'.format('_PCT' if percentage else '')])
    #Partial Total
    worksheet.write_formula(row+3, col+4, '{{=SUM({}:{})}}'.format(xl_rowcol_to_cell(row+3, col+2), xl_rowcol_to_cell(row+3, col+3)), formats['NORMAL{}'.format('_PCT' if percentage else '')])


    #None/Unaltered
    worksheet.write(row+4, col+2, _get_value_or_percentage(result, 'none_unaltered', percentage), formats['NONE/UNALTERED{}'.format('_PCT' if percentage else '')])
    #None/Altered
    worksheet.write(row+4, col+3, _get_value_or_percentage(result, 'none_altered', percentage), formats['NONE/ALTERED{}'.format('_PCT' if percentage else '')])
    #None Total
    worksheet.write_formula(row+4, col+4, '{{=SUM({}:{})}}'.format(xl_rowcol_to_cell(row+4, col+2), xl_rowcol_to_cell(row+4, col+3)), formats['NORMAL{}'.format('_PCT' if percentage else '')])

    #Unaltered Total
    worksheet.write_formula(row+5, col+2, '{{=SUM({}:{})}}'.format(xl_rowcol_to_cell(row+2, col+2), xl_rowcol_to_cell(row+4, col+2)), formats['NORMAL{}'.format('_PCT' if percentage else '')])
    #Altered Total
    worksheet.write_formula(row+5, col+3, '{{=SUM({}:{})}}'.format(xl_rowcol_to_cell(row+2, col+3), xl_rowcol_to_cell(row+4, col+3)), formats['NORMAL{}'.format('_PCT' if percentage else '')])
    #Total
    worksheet.write_formula(row+5, col+4, '{{=SUM({}:{})}}'.format(xl_rowcol_to_cell(row+2, col+4), xl_rowcol_to_cell(row+4, col+4)), formats['NORMAL{}'.format('_PCT' if percentage else '')])


def _get_value_or_percentage(stage_dict, value, percentage=False):

    if percentage is True:
        total_testcases = len(stage_dict['full_unaltered']) + len(stage_dict['full_altered']) \
            + len(stage_dict['partial_unaltered']) + len(stage_dict['partial_altered']) \
            + len(stage_dict['none_unaltered']) + len(stage_dict['none_altered'])

        if total_testcases > 0:
            return float(len(stage_dict[value]))/float(total_testcases)*100.0
        else:
            return 0
    else:
        return len(stage_dict[value])

def write_xlsx_by_cwe(args, workbook,formats, stages, testcases):
    widgets = ['Writing By CWE Sheet: ', Timer(), ' ']
    pbar = ProgressBar(widgets=widgets, maxval=len(testcases)).start()
    summary_sheet = workbook.add_worksheet('Summary By CWE')
    pbar.finish()


def subcommand_args_raw(sub_parsers):

    parser = sub_parsers.add_parser("raw",
                                    help="Get raw scoring results from a queue or all queues.",
                                    description="Get raw scoring results from a queue or all queues.")

    subcommand_args_common(parser)

    parser.add_argument("-q", "--queues", dest="queues",
                        default=["all"], required=False,
                        nargs="+", help="Names of queues to pull scoring results from.")
    parser.add_argument("-t", "--type", dest="type", default="execute", required=False, help="analyze, execute or both")
    parser.add_argument("-s", "--status", dest="status", default="completed", help="completed, uncompleted or both")
    parser.add_argument("-st", "--score_type", dest="score_type", default='all', help="all, overall, formula, or check (only valid for Execute type)")
    parser.add_argument("--perf", dest="perf", action="store_true", default=False, help="output raw performance data")
    parser.add_argument("--time", dest="time_started", default=None, 
                        help="Get all runs with a time started greater than or equal to a specified time. Format: 'DD-MM-YYYY HH:MM:SS'")

    parser.set_defaults(handler=subcommand_raw)

def subcommand_raw(args):
    """
    Dump raw scoring results
    """
    init_db(args)
    init_output(args)
    queues = []
    queue_ids = []
    if args.queues and 'all' not in args.queues :
        queue_ids = database.find(DbQueue.collection(), { 'queue_name' : { '$in' : args.queues } })
    else:
        queue_ids = database.find(DbQueue.collection(), {})

    for queue_id in queue_ids:
            queues.append(DbQueue.load_object(queue_id))


    if args.type.lower() == 'analyze' or args.type.lower() == 'both':
        write_raw_analyze_results(args, queues)

    if args.type.lower() == 'execute' or args.type.lower() == 'both':
        write_raw_execute_results(args, queues, args.cull_valid)

def write_raw_analyze_results(args, queues):
    global analyze_headers

    for queue in queues:
        widgets = ['Analysis - {}: '.format(queue.queue_name), Percentage(), ' ', Bar(),
               ' ', AdaptiveETA(), ' ']

        output_file=os.path.abspath(os.path.join(args.output, '{}-analyze.csv'.format(queue.queue_name)))
        subquery = {}
        analyze_runs = None
        if args.status.lower() == 'completed':
            subquery.update({'result' : { '$ne' : 'null'}})
        elif args.status.lower() == 'uncompleted':
            subquery.update({ 'result' : 'null' })
 
        if args.time_started is not None:
            time_started = convert_time_started(args.time_started)
            subquery.update({'time_started' : {'$gte' : time_started}})


        analyze_runs = queue.get_analyze_runs(subquery)

        pbar = ProgressBar(widgets=widgets, maxval=len(analyze_runs)).start()
        with open(output_file, 'w', 0) as csv_file:
            csv_export_writer = csv.writer(csv_file, delimiter=',',
                         quoting=csv.QUOTE_MINIMAL)
            csv_export_writer.writerow(analyze_headers)

            x=0
            #loop through analyze runs
            for analyze_run in analyze_runs:
                pbar.update(x)
                x += 1
                row = [ analyze_run.testcase.name,
                        'Stage 2' if analyze_run.performer else 'Stage 1',
                        analyze_run.id,
                        _calc_runtime(analyze_run.time_started, analyze_run.time_ended),
                        analyze_run.cwe_found,
                        analyze_run.cwes_found,
                        analyze_run.impact_found,
                        analyze_run.impacts_found,
                        analyze_run.result,
                        analyze_run.error]

                csv_export_writer.writerow(row)
                csv_file.flush()
        pbar.finish()

def write_raw_execute_results(args, queues, cull=True):
    global execute_headers, perf_headers
    for queue in queues:
        output_file=os.path.abspath(os.path.join(args.output, '{}-execute.csv'.format(queue.queue_name)))
        perf_file = '/dev/null'
        if args.perf:
            perf_file=os.path.abspath(os.path.join(args.output, '{}-execute-perf.csv'.format(queue.queue_name)))

        widgets = ['Execute - {}: '.format(queue.queue_name), Percentage(), ' ', Bar(marker=RotatingMarker()),
               ' ', AdaptiveETA(), ' ']

        execute_runs = None
        subquery = {}
        if args.status.lower() == 'completed':
            subquery.update({'result' : { '$ne' : 'null'}})
        elif args.status.lower() == 'uncompleted':
            subquery.update({ 'result' : 'null' })
        

        if args.time_started is not None:
            time_started = convert_time_started(args.time_started)
            subquery.update({'time_started' : {'$gte' : time_started}})
            
        execute_runs = queue.get_execute_runs(subquery)

        execute_runs.sort(key=lambda x: (x._tc_id, x.performer))
        pbar = ProgressBar(widgets=widgets, maxval=len(execute_runs) if len(execute_runs) > 0 else 1).start()
        with open(output_file, 'w', 0) as csv_file, open(perf_file, 'w', 0) as perf_csv_file:
            execute_writer = csv.writer(csv_file, delimiter=',',
                         quoting=csv.QUOTE_MINIMAL)
            perf_writer = csv.writer(perf_csv_file, delimiter=',',
                         quoting=csv.QUOTE_MINIMAL)

            query = {}
            if args.score_type == 'overall':
                query.update({'score_type' : 'overall'})
            elif args.score_type == 'formula':
                query.update({ 'score_type' : 'formula' })
            elif args.score_type == 'check':
                query.update({ 'score_type' : 'check'})
            x=0
            execute_writer.writerow(execute_headers)
            perf_writer.writerow(perf_headers)

            passing_stage_1 = []
            for execute_run in execute_runs:
                pbar.update(x)
                x += 1
                stage = 'Stage 2' if execute_run.performer else 'Stage 1'

                if not execute_run.performer and execute_run.result is True and execute_run.testcase.name not in passing_stage_1:
                        passing_stage_1.append(execute_run.testcase.name)

                if not execute_run.performer or (not cull or (cull and execute_run.performer and execute_run.testcase.name in passing_stage_1)):
                    results = execute_run.get_scoring_results(subquery=query)
                    if len(results) == 0 and (args.score_type == 'overall' or args.score_type == 'all'):
                        execute_row = [execute_run.testcase.name,
                              'Stage 2' if execute_run.analyze_run.performer else 'Stage 1',
                               stage,
                               execute_run.iopair.name,
                               execute_run.iopair.category,
                               execute_run.analyze_run.id,
                               execute_run.id,
                               _calc_runtime(execute_run.analyze_run.time_started, execute_run.analyze_run.time_ended),
                               _calc_runtime(execute_run.time_started, execute_run.time_ended),
                               _calc_external_runtime(execute_run) if args.perf else '',
                               _calc_internal_runtime(execute_run) if args.perf else '',
                               execute_run.cwe_found,
                               ','.join(execute_run.cwes_found),
                               execute_run.impact_found,
                               ','.join(execute_run.impacts_found),
                               execute_run.timeout,
                               'overall',
                               '',
                               '',
                               execute_run.result,
                               execute_run.error,
                               'No']
                        execute_writer.writerow(execute_row)
                        csv_file.flush()
                    else:
                        for result in results:
                            execute_row = [execute_run.testcase.name,
                                   'Stage 2' if execute_run.analyze_run.performer else 'Stage 1',
                                   stage,
                                   execute_run.iopair.name,
                                   execute_run.iopair.category,
                                   execute_run.analyze_run.id,
                                   execute_run.id,
                                   _calc_runtime(execute_run.analyze_run.time_started, execute_run.analyze_run.time_ended),
                                   _calc_runtime(execute_run.time_started, execute_run.time_ended),
                                   _calc_external_runtime(execute_run) if args.perf else '',
                                   _calc_internal_runtime(execute_run) if args.perf else '',
                                   execute_run.cwe_found,
                                   ','.join(execute_run.cwes_found),
                                   execute_run.impact_found,
                                   ','.join(execute_run.impacts_found),
                                   execute_run.timeout,
                                   result.score_type,
                                   result.name,
                                   result.value,
                                   result.result,
                                   result.error,
                                   'No']
                            execute_writer.writerow(execute_row)
                            csv_file.flush()

                            if args.perf:
                                for perf_result in execute_run.get_performance_results({ 'name' : { '$in' :
                                                                                                 ['stonesoup_trace:trace_start',
                                                                                                  'stonesoup_trace:trace_end',
                                                                                                  'stonesoup_trace:weakness_start',
                                                                                                  'stonesoup_trace:weakness_end',
                                                                                                  'stonesoup_trace:trace_location']}}):
                                    perf_result_row = [execute_run.testcase.name,
                                           'Stage 2' if execute_run.performer else 'Stage 1',
                                           execute_run.iopair.name,
                                           execute_run.iopair.category,
                                           execute_run.analyze_run.id,
                                           execute_run.id,
                                           _calc_runtime(execute_run.time_started, execute_run.time_ended),
                                           execute_run.timeout,
                                           perf_result.point_type,
                                           perf_result.ordinal,
                                           perf_result.name,
                                           perf_result.time_stamp,
                                           perf_result.delta]
                                    perf_writer.writerow(perf_result_row)
                                    perf_csv_file.flush()
            pbar.finish()

def convert_time_started(time_started):
    new_time = time.strptime(time_started, "%d-%m-%Y %H:%M:%S")
    return time.mktime(new_time)

def _calc_runtime(starttime, endtime):
    if starttime and endtime:
        return endtime - starttime
    else:
        return None

def _calc_external_runtime(execute_run):
    results_list = execute_run.get_performance_results(subquery={'point_type' : 'External'})
    
    if len(results_list) == 0:
        return 0
    sorted_results = sorted(results_list, key=lambda x: x.ordinal)

    start_time = convert_external_time(sorted_results[0].time_stamp)
    deltas = []
    #get list of deltas
    for result in sorted_results:
        current_time = convert_external_time(result.time_stamp)
        deltas.append(current_time-start_time)

    overall_time = sum(dt.seconds + dt.microseconds * 0.000001 for dt in deltas)

    return overall_time

def _calc_internal_runtime(execute_run):
    perf_results = database.find(collection=DbPerformanceResult.collection(),
                                 query={'execute_id' : execute_run.id,
                                        'point_type' : 'Internal'},
                                 limit=2000,
                                 max_scan=2000)


    ids = list()
    for result in perf_results:
        ids.append(result['_id']) 

    
    results_list = execute_run.get_performance_results(subquery={'_id' : {'$in' : ids},
                                                                 'point_type' : 'Internal'})
    
    if len(results_list) == 0:
        return 0
    sorted_results = sorted(results_list, key=lambda x: x.ordinal)

    start_time = convert_internal_time(sorted_results[0].time_stamp)
    
    deltas = []
    for result in sorted_results:
        current_time = convert_internal_time(result.time_stamp)
        deltas.append(current_time-start_time)

    overall_time = sum(dt.seconds + dt.microseconds * 0.000001 for dt in deltas)

    return overall_time
    
def init_db(args):
    db_addr = args.IP
    db_port = args.PORT
    database.create(db_addr, db_port)

def init_output(args):
    if not os.path.exists(args.output):
        os.makedirs(args.output)

def signal_handler(signal, frame):
    print("This immediately exits TEXAS, which leaves detached processes running.")
    sys.exit(0)

def get_testcases(args, queue):
    """Returns all testcases in a queue
    Unless the user specified testcases, in which case it
    verifies those testcases exist in the queue and returns only a list
    of those that are.
    """

    testcase_list = []
    print "Pulling down testcases.."
    if args.testcases is not None:
        for testcase in args.testcases:
            db_testcase = database.find_one(DbTestcase.collection(),
                                            {'name' : testcase})

            analyze_runs = database.find(DbAnalyzeRun.collection(),
                                         {'tc_id' : db_testcase['_id'],
                                         'queue_id' : queue.id})
            if analyze_runs.count() > 0:
                testcase_list.append(db_testcase['_id'])


    else:
        analyze_runs = database.find(DbAnalyzeRun.collection(),
                                     {'queue_id' : queue.id})

        for run in analyze_runs:
            testcase_list.append(run['tc_id'])


    return testcase_list


def get_performance_results(queue, testcase_list):
    """Query the database for the performance results based on execution id
    this query is similar to:
    SELECT execute_id, sum(runs) as count, list(ids) as internal, list(ids) as external
    FROM performance_results
    WHERE execute_id in execute_ids
    GROUP BY execute_id
    - internal and external are lists of performance_result objects
      reduced by mongo into each respective list based on the point_type.
    """
    print "Querying database.."
    reducer_func = "function (curr, result) { \
            result.count++; \
            if (curr.name == 'stonesoup_trace:trace_start' || \
            curr.name == 'stonesoup_trace:trace_end'){ if (true){ result.internal.push(curr._id);} } \
            else if (curr.name == 'stonesoup_trace:weakness_start' ||\
            curr.name == 'stonesoup_trace:weakness_end' || curr.name == 'stonesoup_trace:trace_location'){ result.internal.push(curr._id); }\
            else if (curr.point_type == 'External') { result.external.push(curr._id); } \
            else { result.count-- } \
            }"


    finalizer_func = "function (result) { \
            return result; \
            }"
    execute_runs = database.find(DbExecuteRun.collection(),
                                 {'queue_id' : queue.id,
                                  'tc_id' : {'$in' : testcase_list}})

    execute_id_list = execute_runs.distinct('_id')
    pbar = DownloadTimer(widget_string="Querying")
    pbar.start()
    #gets a list of all internal and external points per iopair run.
    perf_result_list = database.group(collection=DbPerformanceResult.collection(),
                                      key={'execute_id' : 1},
                                      condition={'execute_id' : {'$in' : execute_id_list}},
                                      initial={'internal' : [], 'external' : [], 'count' : 0},
                                      reducer=reducer_func,
                                      finalizer=finalizer_func)

    pbar.stop()
    return perf_result_list



def organize_results(performance_results):
    """Organize the results by testcase, such that
    each testcase contains a dictionary of GOOD io-pairs,
    and within that a list of dictionaries on each run.
    Ex:
        {J-TREE : {
                   GOOD_01 : [ {<run 1>}, {<run 2>} ],
                   GOOD_02 : [ {<run 1>}, {<run 2>} ]
                  }
        }
    """
    print "Organizing Results"
    widgets = get_default_widgets()
    maxval=len(performance_results)
    pbar = ProgressBar(widgets=widgets,maxval=maxval).start()

    organized_testcases = dict()
    x = 0
    for result in performance_results:
        execute_run = DbExecuteRun.load_object(result['execute_id'])
        if not execute_run.result is True:
            continue
        testcase_name = execute_run.testcase.name

        if "BAD" in execute_run.iopair.name:
            continue
        iopair = execute_run.iopair.name
        stage1="-STAGE1"
        stage2="-STAGE2"

        if execute_run.performer:
            iopair=iopair + stage2
        else:
            iopair=iopair + stage1

        if testcase_name not in organized_testcases:
            organized_testcases[testcase_name] = {}
            organized_testcases[testcase_name][iopair] = [result]

        else:
            if iopair not in organized_testcases[testcase_name]:
                organized_testcases[testcase_name][iopair]= []

            organized_testcases[testcase_name][iopair].append(result)
        pbar.update(x)
        x += 1
    sort_runs(organized_testcases)
    pbar.finish()
    return organized_testcases


def sort_runs(organized_results):
    """Calculate overall averages for each testcase iopair.
    Checks to see if we're calculating the average for a stage1 or stage 2
    run. Calculate the averages for stage 1 runs normally, and for stage 2
    calculate the average for the runs and then compare that to stage 1
    to determine overhead of performer technology.
    """


    #for each testcase, get each io_pair and
    #calculate average of each point within that iopair.
    #inline replace as well!
    for testcase in organized_results.keys():
        for iopair in organized_results[testcase].keys():
            sorted_perf_results = sort_results(organized_results[testcase][iopair])
            organized_results[testcase][iopair] = sorted_perf_results



def sort_results(iopair_run_results):
    internal_runs = []
    external_runs = []

    for run in iopair_run_results:
        internal = get_perf_results(run['internal'])
        external = get_perf_results(run['external'])


        internal = sorted(internal,
                          key=lambda x: x.ordinal)

        external = sorted(external,
                          key=lambda x: x.ordinal)

        internal_runs.append(internal)
        external_runs.append(external)



    return {'internal' : internal_runs,
            'external' : external_runs}


def get_perf_results(results_list):
    db_list = []

    for result in results_list:
        perf_result = DbPerformanceResult.load_object(result)
        if perf_result is not None:
            db_list.append(perf_result)

    return db_list


def calculate_spreadsheet(organized_results):
    widgets = get_default_widgets()

    print "Calculating run averages."
    maxval = len(organized_results.keys())
    pbar = ProgressBar(widgets=widgets, maxval=maxval).start()
    x = 0
    for testcase in organized_results.keys():
        for iopair in sorted(organized_results[testcase].keys()):
            averaged_iopair = average_iopair_results(organized_results[testcase][iopair])
            organized_results[testcase][iopair] = averaged_iopair
            if "STAGE2" in iopair:
                stage1 = iopair.replace("STAGE2", "STAGE1")
                stage1_final, stage2_final = calculate_overhead(organized_results[testcase][stage1],
                                                                averaged_iopair)
                organized_results[testcase][stage1] = stage1_final
                organized_results[testcase][iopair] = stage2_final
        pbar.update(x)
        x+=1
    pbar.finish()

def average_iopair_results(iopair_results):

    internal_deltas = calculate_internal_deltas(iopair_results['internal'])
    external_deltas = calculate_external_deltas(iopair_results['external'])

    average_dict = {'internal' : calc_delta_avg(internal_deltas),
                    'external' : calc_delta_avg(external_deltas)}

    return average_dict

def calculate_run_overhead(stage1, stage2):
    max_length = 0
    if len(stage1) > len(stage2):
        max_length = len(stage1)
    elif len(stage1) < len(stage2):
        max_length = len(stage2)
    else:
        max_length = len(stage1)

    stage1_final = []
    stage2_final = []
    for i in range(max_length):
        stage1_final.append(stage1[i])
        stage1_final.append('')
        stage2_final.append(stage2[i])
        stage2_final.append(stage2[i]-stage1[i])

    return stage1_final, stage2_final



def calculate_overhead(stage1_iopair, stage2_iopair):
    stage1_internal, stage2_internal = calculate_run_overhead(stage1_iopair['internal'],
                                                              stage2_iopair['internal'])

    stage1_external, stage2_external = calculate_run_overhead(stage1_iopair['external'],
                                                              stage2_iopair['external'])


    stage1 = {'internal' : stage1_internal,
              'external' : stage1_external}
    stage2 = {'internal' : stage2_internal,
              'external' : stage2_external}
    return stage1, stage2

def calculate_internal_deltas(internal_runs):
    internal_list = []
    for run in internal_runs:
        current_point = None
        point_deltas = []
        for point in run:
            if current_point is None:
                current_point = point
                point_deltas.append(0)
                continue
            else:
                previous_stamp = convert_internal_time(current_point.time_stamp)
                current_stamp = convert_internal_time(point.time_stamp)
                delta = current_stamp - previous_stamp
                current_point = point
                point_deltas.append(delta)

        internal_list.append(point_deltas)
    return internal_list

def calculate_external_deltas(external_runs):
    external_list = []
    for run in external_runs:
        current_point = None
        point_deltas = []
        for point in run:
            if current_point is None:
                current_point = point
                point_deltas.append(0)
            else:
                previous_stamp = convert_external_time(current_point.time_stamp)
                current_stamp = convert_external_time(point.time_stamp)
                delta = current_stamp - previous_stamp
                current_point = point
                point_deltas.append(delta)
        external_list.append(point_deltas)

    return external_list

def calc_delta_avg(internal_deltas):
    point_avg_list = []
    #start by getting the max length of trace points for a given iopair
    max_length = max(len(l) for l in internal_deltas)
    for i in range(max_length):
        point_avg_list.append([])
    for run in internal_deltas:

        if len(run) == max_length:
            for i in range(max_length):
                point_avg_list[i].append(run[i])
        else:
            for i in range(len(run)):
                point_avg_list[i].append(run[i])

    for i in range(len(point_avg_list)):
        point = point_avg_list[i]
        if isinstance(point[0], timedelta):
            point_sum = sum(dt.seconds + dt.microseconds * 0.000001 for dt in point)
            point_avg_list[i] = point_sum/float(len(point))
        else:
            point_sum = sum(point)
            point_avg_list[i] = point_sum/ float(len(point))

    return point_avg_list

def convert_internal_time(timestamp):
    #internal timestamps all follow H:M:S.mmmmm format
    #The data we get from the database extends the ms
    #too far for good precision. We have to chop that off.
    if len(timestamp) > 15:
        timestamp = timestamp[0:15]

    date_timestamp = None
    if '.' in timestamp:
        date_timestamp = datetime.strptime(timestamp, "%H:%M:%S.%f")
    else:
        date_timestamp = datetime.strptime(timestamp, "%H:%M:%S")

    #return a timestamp, for simple editing later
    #It makes no sense to return a float,
    #since datetime will provide an easy interface to find differences later.
    return date_timestamp



def convert_external_time(timestamp):
    '''External timestamps come out as " 2014-01-01 01:01:01.010101" (note the whitespace at the beginning)
    or as a string version of a float ex: "14027174.2141"
    So we convert them and then try to return a value that works.
    A nice feature: External timestamps don't have to be reduced to
    15 characters like internal timestamps. Quite nifty
    '''
    date_timestamp = None
    #date timestamp
    if '-' in timestamp:
        timestamp = timestamp.split(' ')[2]

        if '.' in timestamp:
            date_timestamp = datetime.strptime(timestamp, "%H:%M:%S.%f")
        else:
            date_timestamp = datetime.strptime(timestamp, "%H:%M:%S")
    else:
        #float timestamp
        timestamp = float(timestamp)
        date_timestamp = datetime.fromtimestamp(timestamp)



    return date_timestamp

def write_spreadsheet(workbook, formats, organized_results):
    external_worksheet = workbook.add_worksheet('External Performance Results')
    internal_worksheet = workbook.add_worksheet('Internal Performance Results')

    row = 1
    for testcase in organized_results.keys():
        col = 0
        external_worksheet.write_string(row, col, testcase)
        external_worksheet.write_string(row, col + 1, "Stage")
        internal_worksheet.write_string(row, col, testcase)
        internal_worksheet.write_string(row, col + 1, "Stage")
        max_len_internal = max(len(organized_results[testcase][p]['internal']) \
                                       for p in organized_results[testcase].keys())
        max_len_external = max(len(organized_results[testcase][p]['external']) \
                                       for p in organized_results[testcase].keys())
        stage2=False
        for key in organized_results[testcase].keys():
            if "STAGE2" in key:
                stage2 = True
                break
        col+=2
        if stage2:
            x = 0
            for i in range(0, max_len_internal, 2):
                internal_worksheet.write_string(row, col + i, "Point {}".format(x))
                internal_worksheet.write_string(row, col + i + 1, "Point {} Overhead".format(x))
                x+=1
            x = 0
            for i in range(0, max_len_external, 2):
                external_worksheet.write_string(row, col + i, "Point {}".format(x))
                external_worksheet.write_string(row, col + i + 1, "Point {} Overhead".format(x))
                x += 1
            internal_worksheet.write_string(row, col + max_len_internal , "Total Runtime")
            internal_worksheet.write_string(row, col + max_len_internal + 1, "Stage 2 Overhead")
            external_worksheet.write_string(row, col + max_len_external , "Total Runtime")
            external_worksheet.write_string(row, col + max_len_external + 1, "Stage 2 Overhead")
        else:
            for i in range(0, max_len_internal):
                internal_worksheet.write_string(row, col + i, "Point {}".format(i))
            for i in range(0, max_len_external):
                external_worksheet.write_string(row, col+i, "Point {}".format(i))

            internal_worksheet.write_string(row, col + max_len_internal , "Total Runtime")
            external_worksheet.write_string(row, col + max_len_external , "Total Runtime")

        for iopair in sorted(organized_results[testcase].keys()):
            row += 1
            col = 0
            stage_text = ""
            if "STAGE1" in iopair:
                stage_text = "Stage 1"
            else:
                stage_text = "Stage 2"
            iopair_name = "{}".format("-".join(iopair.split('-')[:-1]))
            internal = organized_results[testcase][iopair]['internal']
            external = organized_results[testcase][iopair]['external']
            external_worksheet.write_string(row, col, iopair_name)
            internal_worksheet.write_string(row, col, iopair_name)
            internal_worksheet.write_string(row, col + 1, stage_text)
            external_worksheet.write_string(row, col + 1, stage_text)
            internal_col = 2
            external_col = 2

            for result in internal:
                if isinstance(result, basestring):
                    internal_worksheet.write_string(row, internal_col, result)
                else:
                    internal_worksheet.write_number(row, internal_col, result, formats['TIMESTAMP'])

                internal_col += 1

            for result in external:
                if isinstance(result, basestring):
                    external_worksheet.write_string(row, external_col, result)
                else:
                    external_worksheet.write_number(row, external_col, result, formats['TIMESTAMP'])

                external_col += 1
            internal_total, internal_overhead = get_internal_totals(organized_results,testcase, iopair, stage2)
            external_total, external_overhead = get_external_totals(organized_results,testcase, iopair, stage2)
            internal_worksheet.write_number(row, col + max_len_internal +2, internal_total)
            external_worksheet.write_number(row, col + max_len_external +2, external_total)
            if stage2:
                internal_worksheet.write_number(row, col + max_len_internal + 3, internal_overhead)
                external_worksheet.write_number(row, col + max_len_external + 3, external_overhead)

        row += 2


def get_internal_totals(organized_results, testcase, iopair, stage2):
    internal = organized_results[testcase][iopair]['internal']
    if stage2:
        if "STAGE1" in iopair:
            n = 0
            for i in range(0, len(internal), 2):
                n += internal[i]

            return n, 0
        else:
            s1 = iopair.replace("STAGE2", "STAGE1")
            s1_internal = organized_results[testcase][s1]['internal']
            s1_sum = 0
            for i in range(0, len(s1_internal), 2):
                s1_sum += s1_internal[i]
            inter_sum = 0
            for i in range(0, len(internal), 2):
                inter_sum += internal[i]

            return inter_sum, inter_sum - s1_sum
    else:
        n = 0
        for i in range(len(internal)):
            n += internal[i]
        return n, 0

def get_external_totals(organized_results, testcase, iopair, stage2):
    external = organized_results[testcase][iopair]['external']
    if stage2:
        if "STAGE1" in iopair:
            n = 0
            for i in range(0, len(external), 2):
                n += external[i]

            return n, 0
        else:
            s1 = iopair.replace("STAGE2", "STAGE1")
            s1_external = organized_results[testcase][s1]['external']
            s1_sum = 0
            for i in range(0, len(s1_external), 2):
                s1_sum += s1_external[i]
            exter_sum = 0
            for i in range(0, len(external), 2):
                exter_sum += external[i]

            return exter_sum, exter_sum - s1_sum
    else:
        n = 0
        for i in range(len(external)):
            n += external[i]
        return n, 0



def main():
    signal.signal(signal.SIGINT, signal_handler)
    parser = argparse.ArgumentParser(prog="database_results")

    sub_parsers = parser.add_subparsers(title='commands',
            description='The following commands are supported with this package.\
            Additional options are available for each subcommand. A listing can be\
            obtained using the subcommand and the -h flag.')

    sub_commands = ['raw', 'xlsx']


    for sub_command in sub_commands:
        sub_command_builder = getattr(
            local_module, 'subcommand_args_%s' % sub_command.replace('-', '_'),
            subcommand_args_not_implemented)
        sub_command_builder(sub_parsers)


    args = parser.parse_args()


    args.handler(args)


if __name__ == "__main__":
    main()
